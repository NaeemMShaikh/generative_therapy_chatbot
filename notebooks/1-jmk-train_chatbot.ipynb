{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7LoMj4GA4n_"
   },
   "source": [
    "#  Train a GPT-2 Text-Generating Model w/ GPU For Free \n",
    "\n",
    "by [Max Woolf](http://minimaxir.com)\n",
    "\n",
    "*Last updated: November 10th, 2019*\n",
    "\n",
    "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
    "\n",
    "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read my [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
    "\n",
    "\n",
    "To get started:\n",
    "\n",
    "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
    "2. Make sure you're running the notebook in Google Chrome.\n",
    "3. Run the cells below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBkpRgBCBS2_"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "!pip install -q gpt-2-simple\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wXB05bPDYxS"
   },
   "source": [
    "## Downloading GPT-2\n",
    "\n",
    "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
    "\n",
    "There are three released sizes of GPT-2:\n",
    "\n",
    "* `124M` (default): the \"small\" model, 500MB on disk.\n",
    "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
    "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
    "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
    "\n",
    "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
    "\n",
    "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
    "\n",
    "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9872,
     "status": "ok",
     "timestamp": 1583364279189,
     "user": {
      "displayName": "Jason Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64",
      "userId": "07126822508397489067"
     },
     "user_tz": 300
    },
    "id": "P8wSlgXoDPCR",
    "outputId": "aa7f3dec-9589-4edf-a81f-9a21b6460d0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 420Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 95.9Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 805Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:08, 55.6Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 706Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 135Mit/s]                                                 \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 155Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "gpt2.download_gpt2(model_name=\"124M\") # switch to medium model (or better if colab is upgraded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8KXuKWzQSsN"
   },
   "source": [
    "## Mounting Google Drive and Change Working Directory\n",
    "\n",
    "\n",
    "> The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
    "\n",
    "> Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2409,
     "status": "ok",
     "timestamp": 1583365430042,
     "user": {
      "displayName": "Jason Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64",
      "userId": "07126822508397489067"
     },
     "user_tz": 300
    },
    "id": "puq4iC6vUAHc",
    "outputId": "4bcc530c-8489-4807-bf6e-7cdf10efd54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-h2sN2aM4g9g"
   },
   "source": [
    "# Import files for fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OFnPCLADfll"
   },
   "outputs": [],
   "source": [
    "# Change the current working directory\n",
    "transcript_file_location = \"processed_transcripts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 742
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 234,
     "status": "error",
     "timestamp": 1583365435935,
     "user": {
      "displayName": "Jason Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64",
      "userId": "07126822508397489067"
     },
     "user_tz": 300
    },
    "id": "JMQ6pzCo4nSw",
    "outputId": "43dfcb29-569d-45fa-b230-d8919192d6c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-33-efacdae2ab0b>\", line 1, in <module>\n",
      "    print(os.getcwd())\n",
      "OSError: [Errno 107] Transport endpoint is not connected\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 383, in abspath\n",
      "    cwd = os.getcwd()\n",
      "OSError: [Errno 107] Transport endpoint is not connected\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 428,
     "status": "error",
     "timestamp": 1583365275819,
     "user": {
      "displayName": "Jason Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64",
      "userId": "07126822508397489067"
     },
     "user_tz": 300
    },
    "id": "-Z6okFD8VKtS",
    "outputId": "a396ff2a-d7b7-4fe7-c06d-d516bfc4512e"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0fe0bf4be80c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranscripts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_file_from_gdrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_file_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mcopy_file_from_gdrive\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0mis_mounted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive//content/drive/My Drive/5_datascience/2_context_chatbot/processed_transcripts.txt'"
     ]
    }
   ],
   "source": [
    "transcripts = gpt2.copy_file_from_gdrive(transcript_file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LdpZQXknFNY3"
   },
   "source": [
    "## Finetune GPT-2\n",
    "\n",
    "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
    "\n",
    "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
    "\n",
    "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
    "\n",
    "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
    "\n",
    "\n",
    "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
    "* **`sample_every`**: Number of steps to print example output\n",
    "* **`print_every`**: Number of steps to print training progress.\n",
    "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
    "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
    "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aeXshJM-Cuaf",
    "outputId": "8884ffbb-7c04-4460-f642-cdfd5cddb516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:04<00:00,  4.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 1096225 tokens\n",
      "Training...\n",
      "[10 | 38.44] loss=2.99 avg=2.99\n",
      "[20 | 70.46] loss=2.70 avg=2.84\n",
      "[30 | 102.56] loss=2.80 avg=2.83\n",
      "[40 | 134.65] loss=2.82 avg=2.83\n",
      "[50 | 166.85] loss=2.81 avg=2.82\n",
      "[60 | 199.01] loss=2.63 avg=2.79\n",
      "[70 | 231.25] loss=2.48 avg=2.75\n",
      "[80 | 263.62] loss=2.76 avg=2.75\n",
      "[90 | 296.02] loss=2.85 avg=2.76\n",
      "[100 | 328.39] loss=2.47 avg=2.73\n",
      "[110 | 360.78] loss=2.59 avg=2.71\n",
      "[120 | 393.22] loss=2.66 avg=2.71\n",
      "[130 | 425.58] loss=2.57 avg=2.70\n",
      "[140 | 457.99] loss=2.40 avg=2.68\n",
      "[150 | 490.39] loss=2.75 avg=2.68\n",
      "[160 | 522.74] loss=2.70 avg=2.68\n",
      "[170 | 555.09] loss=2.75 avg=2.69\n",
      "[180 | 587.52] loss=2.58 avg=2.68\n",
      "[190 | 619.88] loss=2.64 avg=2.68\n",
      "[200 | 652.26] loss=2.80 avg=2.69\n",
      "======== SAMPLE 1 ========\n",
      " and like yeah. It's all very weird. It's so different than when I was younger. It's like like the same sort of...but I was just... I'm not a kid or an artist, I'm not even a poet. I'm like I have this thing, and it's this way. It's all these pieces, you know? Anyway, I was like oh, yeah. And what is this, like wow, I remember that whole fucking conversation, when I was in kindergarten, that conversation, how were you? Or did you write a poem? That just got... I don't I'm like that sounds kind of stupid, I don't know. Why didn't I do it? You know? Or did you really? If... It's weird, but I... I was like, wow. It makes me feel like it's just part of me. It's just, but you know what? Whatever. Like it was so great to be able to kind of be like wow, that was so cool. You know what I mean? Like yeah. Yeah, the feeling of, you know, a little bit of the same thing. I mean it wasn't so much in my head, but what is it like if you're in a... It's like when you're in your... When you're not, I mean I guess not like...I don't know. Like, what a weird feeling to be like, if I was a little more like that. It's great to be able to... I mean my parents were like I want to be a poet, but I'm not in... it's not... I don't know... I don't know. But it was really fun. I was like this really weird thing, where I was like really into this thing, which is actually just part of me. It's more a social thing, you know? Like I just feel like this is what I'm going to do someday. Like it's like, this isn't as exciting as I think it is, but... Well... Yeah. That's why I like that you know? Like I don't know. I do, and like you know what? I think I'm done, and then there's nothing to do but... but I will always be like it's cool to just feel like a poet. I just want people to know. I have the most amazing... And it's so cool, man. Not... I don't know, just wow, I feel like, wow. You know what? I don't know, maybe that's why I'm here. Yeah, they're just... I mean like, oh it's just so much better than this. But they're not as bad as you know, it's just just that I'm like how is that for an artist? Like I don't know, but there is... There was just so much better stuff happening in the world. It's cool. But you know what? I... I just feel like I want to write a couple poems or something, you know? Like if I can come out with something, I think that's awesome. But I will be like I know... I'll just make sure everyone's up to it. I mean I think that's all because there's just so much more of... you know? You know, I'm just... Like it's a new world. It's going... I'm so glad I came through this, but I just... I don't know. But people, and they're like this is something that would... just wow it's cool. Like they're like wow, I'm so excited. Yeah, I... it is the... People say my mom's the youngest artist... Like it's actually like her... her mom has her own website. People are like wow, that's awesome. They're like what the? I... just wow, that's so... It's... I was like that really? That's awesome. Like that is such a good start. I could get out to... Do you know you don't, where I'm from, but I would love to come up and do more, like this is so weird. People want to be like wow man, is that amazing. There's some weird fucking thing about it. haha Just wow, I do think people, I don't think it's a bad start, but I can feel a little more and a little bit nervous. Like if I... There's a lot of music out there, now I just wish things were a little more... Do you know what I mean? It's so new, so it's almost a new art form. Like it's all so much more. It's just... I'm just so excited. I'm scared I just can't get out there. That's so weird. Like I don't know. It feels, it feels like there's nothing wrong with wanting people to really be like me. You know what I mean? Like I'm not...\n",
      "\n",
      "[210 | 698.97] loss=2.77 avg=2.69\n",
      "[220 | 731.32] loss=2.68 avg=2.69\n",
      "[230 | 763.72] loss=2.66 avg=2.69\n",
      "[240 | 796.16] loss=2.49 avg=2.68\n",
      "[250 | 828.60] loss=2.60 avg=2.68\n",
      "[260 | 861.02] loss=2.43 avg=2.66\n",
      "[270 | 893.42] loss=2.56 avg=2.66\n",
      "[280 | 925.82] loss=2.68 avg=2.66\n",
      "[290 | 958.22] loss=2.44 avg=2.65\n",
      "[300 | 990.60] loss=2.75 avg=2.66\n",
      "[310 | 1022.95] loss=2.48 avg=2.65\n",
      "[320 | 1055.36] loss=2.37 avg=2.64\n",
      "[330 | 1087.78] loss=2.56 avg=2.64\n",
      "[340 | 1120.19] loss=2.54 avg=2.63\n",
      "[350 | 1152.61] loss=2.70 avg=2.63\n",
      "[360 | 1185.09] loss=2.43 avg=2.63\n",
      "[370 | 1217.55] loss=2.27 avg=2.62\n",
      "[380 | 1250.01] loss=2.33 avg=2.61\n",
      "[390 | 1282.49] loss=2.25 avg=2.60\n",
      "[400 | 1314.96] loss=2.63 avg=2.60\n",
      "======== SAMPLE 1 ========\n",
      " yeah. I'm like ??Oh God bless you, that would be like that when I'm feeling good. So that's a good thing. So that's ??\t\n",
      "CLIENT: And then my mother was like ??This is ?? like, that's amazing. There's a way for you, too.\t\n",
      "THERAPIST: Yeah. I thought about it.\t\n",
      "CLIENT: Yeah, yeah.\t\n",
      "THERAPIST: And it was the other idea. Like maybe a week later ??\t\n",
      "CLIENT: Right, right. Right, right.\t\n",
      "THERAPIST: So I don't know what it was but maybe you were already feeling very tired or like this would happen? Why ??\t\n",
      "CLIENT: No, not the way my mother told it that night.\t\n",
      "THERAPIST: But she just felt ??\t\n",
      "CLIENT: No, I was just feeling out that something wasn't quite right.\t\n",
      "THERAPIST: But that there was something that wasn't\t\n",
      "CLIENT: I was just feeling ?? just feeling like, you know, things are good.\t\n",
      "THERAPIST: I think that's where you were on Monday.\t\n",
      "CLIENT: That's totally true. No, no, well I was just feeling like things were, you know, okay.\t\n",
      "THERAPIST: Yeah I was feeling, whatever.\t\n",
      "CLIENT: I was just feeling like things were really good. The music was great. It was just just, all good stuff. And then it was also funny to see that I guess I guess also, when I was just thinking about my mother's reaction, I feel like she's kind of like, yeah ??what are you listening to? Do you want to hear it? There's something about that. But then she knows a little bit because my mom says she might even watch it if you hear it. And yeah, that's ?? haha I don't, she'll make comments like that. I don't care if she knows me or something, but she's like, yes, if you really don't care. If it's just a really good song, which it is, obviously. If you care about it enough to listen to, but if you care about it enough to see it. So we're just ?? it's so ?? but as I say, I guess if you're just not very communicative.\t\n",
      "THERAPIST: Yeah, for example saying, ??I'm sorry you lost it. Do you give your son a ?? it's great. If your son really needs him, he needs you.? So I think that's more that ?? or more the same stuff about her comment about the radio show. She's like, look, I think we could do even better. Yeah, no, I'd love to hear that. Like maybe it'll be part of our life together, but it would be ??\t\n",
      "CLIENT: Right.\t\n",
      "THERAPIST: That's what I mean ??\t\n",
      "CLIENT: Yeah, yeah, really. Yeah, and also, of course she was right. She just got like, she's trying to say something in the middle of the night. She's trying to say something to the other people watching. So even if they don't hear the song ?? at the desk or in a bar ?? it's all good. It's all good. You know what I mean?\t\n",
      "THERAPIST: She's not saying that it should be on radio.\t\n",
      "CLIENT: Yeah.\t\n",
      "THERAPIST: It would be like she's also saying not that it's not good but when you're talking about your friends they can't hear it so she's also saying all that. So now she's like, why is it a weakness to say it to other people, when you know you're getting a lot from them and she knows so little?\t\n",
      "CLIENT: She could be making a comment but she has to acknowledge that I know my strengths even though I'm not ??\t\n",
      "THERAPIST: So she's saying ??\t\n",
      "CLIENT: That would be a weakness to say all this. I don't know what she's going on about. It's like, it's like I don't know how she's right. I could be ?? yeah, in terms of like I don't mind saying it.\t\n",
      "THERAPIST: Yeah it would be ??\t\n",
      "CLIENT: But a lot of times I would like just kind of ?? I wouldn't want to use her name. I think that's the best I could ?? her best, I mean, she has a sweet voice, it's ?? I mean, she's very ?? it's a shame, I think. She's an artist. And she plays music great. She plays her music great, like ?? well she's, she's just like a true believer. I know, maybe it's just that she knows ?? yeah, I know,\n",
      "\n",
      "[410 | 1360.69] loss=2.62 avg=2.60\n",
      "[420 | 1393.11] loss=2.57 avg=2.60\n",
      "[430 | 1425.50] loss=2.40 avg=2.59\n",
      "[440 | 1457.99] loss=2.39 avg=2.59\n",
      "[450 | 1490.35] loss=2.74 avg=2.59\n",
      "[460 | 1522.77] loss=2.66 avg=2.59\n",
      "[470 | 1555.25] loss=2.49 avg=2.59\n",
      "[480 | 1587.62] loss=2.37 avg=2.58\n",
      "[490 | 1620.07] loss=2.32 avg=2.58\n",
      "[500 | 1652.50] loss=2.43 avg=2.57\n",
      "Saving checkpoint/run1/model-500\n",
      "[510 | 1687.80] loss=2.34 avg=2.57\n",
      "[520 | 1720.23] loss=2.48 avg=2.57\n",
      "[530 | 1752.62] loss=2.53 avg=2.56\n",
      "[540 | 1785.00] loss=2.44 avg=2.56\n",
      "[550 | 1817.37] loss=2.26 avg=2.55\n",
      "[560 | 1849.80] loss=2.47 avg=2.55\n",
      "[570 | 1882.20] loss=2.45 avg=2.55\n",
      "[580 | 1914.64] loss=2.45 avg=2.55\n",
      "[590 | 1947.09] loss=2.28 avg=2.54\n",
      "[600 | 1979.51] loss=2.28 avg=2.54\n",
      "======== SAMPLE 1 ========\n",
      ". It can't go there! There's so much we don't have to talk to other people that it's just not possible...\t\n",
      "THERAPIST: So what next?\t\n",
      "CLIENT: It's so late. But, yeah. I'm having trouble with that. I'm feeling kind of stressed out about it because I don't feel quite like a direct connection to the person but I'm feeling like a direct connection somehow. I'm not just a romantic but intimate relation with this person or this way. I don't know.\t\n",
      "THERAPIST: That's not the way it feels.\t\n",
      "CLIENT: No, no, no, I don't feel that way! Or it's even like, yeah, I know we've really talked about dating or things like that but...\t\n",
      "THERAPIST: Well it's not about getting into a relationship with someone who is your actual romantic interest.\t\n",
      "CLIENT: I'm just saying it just seems so clear and obvious and yet I'm also saying that there's another level of, it's like I'm not like, I don't feel that way! I'm not a super attractive person. I don't think it's because I'm a guy where I'm not flirting. Do you know what I mean? I have this much in common with you but I can also relate...I'll be perfectly honest with you about that too. You know what I mean? Like I just...I love my job and I love my kids. I don't fucking know. I'm super into things. I just don't it bothers me. I'm super excited about this job. This job sounds really good to me so I feel like, you know, I'm kind of like the center-of-the-rock type. Do you know what I mean? Like I'm going to do stuff I love. I'm super excited about this job. Like I don't know...I don't know. There's a lot of music critics who have said it's like, you know what? This is, I almost feel like, you know, I'm not saying that there's a fucking Jesus Christ of a job. Do you know what I mean? These people could be saying whatever, man, why would you do this to them? It's like, so is that why, in a way...I still do some freelance writing. But...I also feel like I'm a bit underskirt, you know? I did some kind of speaking, speaking for thing or event and they asked me, is this really the way you do you know what I mean? Do you know what I mean? I don't know, this is why I'm saying it's like, I want to be in this room for four years writing. I want to write and I want to connect and be interesting and artistic and be, all that jazz. I'm not like that today but I do feel like I'm in it for the next 4 years or whatever. I might not do that, this is like a good...I can't over...it's like, you know how much better I feel when I do things in my own way. I'm really proud of it. It's like, you know what? I'm not an asshole, I'm just, you know what? My website has 300,000 likes, millions views and that's not some bullshit...it's just my own way of approaching things, you know what I mean? Like it's just the way I feel. Yeah, it sucks but I'm not trying to take it lightly when I'm feeling better or, you know, it's that it's all online...do you know what I mean? Do you know what I mean? Because of that, that's why I've been writing. Because that's what I think I'll do now is I'm gonna be writing. It's like, I'm just going to be...if I'm going to be doing this I've got to be writing for an artist. Do you know what I mean? There's just always that one.\t\n",
      "THERAPIST: The way you're speaking is different.\t\n",
      "CLIENT: Exactly.\t\n",
      "THERAPIST: You're...\t\n",
      "CLIENT: I'm a little bit more hip hop about it, yeah, like hip hop is about life, you know what I mean? Like the more you're like, ??fuck, now you've got to, you know, what are you doing?' And it's an old school hip hop kind of...but, yeah, I'm doing it just...so I'm not like the guy that's fucking living it up at night in a shitty studio in a shitty city. It's about life. It's more about not living up to...who the fuck cares, anyway? I do I do that a lot now but then it's not about, I'll be living in a fucking shitty loft or whatever. It's I\n",
      "\n",
      "[610 | 2025.18] loss=2.31 avg=2.53\n",
      "[620 | 2057.61] loss=2.25 avg=2.53\n",
      "[630 | 2090.05] loss=2.32 avg=2.52\n",
      "[640 | 2122.47] loss=2.25 avg=2.52\n",
      "[650 | 2154.87] loss=2.39 avg=2.51\n",
      "[660 | 2187.30] loss=2.23 avg=2.51\n",
      "[670 | 2219.70] loss=2.41 avg=2.50\n",
      "[680 | 2252.16] loss=2.46 avg=2.50\n",
      "[690 | 2284.59] loss=2.24 avg=2.50\n",
      "[700 | 2317.02] loss=2.29 avg=2.49\n",
      "[710 | 2349.47] loss=2.14 avg=2.49\n",
      "[720 | 2381.98] loss=2.09 avg=2.48\n",
      "[730 | 2414.41] loss=2.32 avg=2.48\n",
      "[740 | 2446.93] loss=2.09 avg=2.47\n",
      "[750 | 2479.33] loss=2.35 avg=2.47\n",
      "[760 | 2511.76] loss=2.18 avg=2.46\n",
      "[770 | 2544.22] loss=2.48 avg=2.46\n",
      "[780 | 2576.62] loss=2.02 avg=2.45\n",
      "[790 | 2609.07] loss=2.03 avg=2.45\n",
      "[800 | 2641.57] loss=2.10 avg=2.44\n",
      "======== SAMPLE 1 ========\n",
      " it doesn??t seem to be what??s so hard about it. I feel like there is some complexity around that that goes with the question, which is more like, you know, is there a kind of reality to all this, or is it more just about feeling good or just a fantasy which you sort of see, and maybe there??s fantasy or something, or this, but it??s an ordinary thing. But I don??t see it as a part of the dynamic that I??m having. So I think that??s part of the problem. Or maybe part of the problem as well. You know, I think that maybe there are aspects of that, or we??re both kind of, but also... I don??t know. I don??t know. Like I??m like, you know, I??m just kind of tired, I guess, kind of kind of. But, I mean, again, like I said, I think that there is some reality to the narrative that you have with everything, that??s a whole other story. But it??s not really why I was going to stay for the day, or why I was sleeping so well, or why I was depressed. I just think that maybe I think that I am kind of just kind of tired and just kind of not where I want to be.\t\n",
      "THERAPIST: Mm-hmm.\t\n",
      "CLIENT: You know... I guess what I was intending to say was, you know, I think I??m just kind of like, I don??t know, we need to just put it aside for the moment. I mean, I think that what we??re talking about today, I think clearly, is structural. It??s also that, even as I feel kind of content and more confident and something, that I don??t know, about how to respond to certain things that I have done and how to respond in other contexts, like today, this was a very, very difficult day. And yet, I don??t feel like I was, you know, in touch with, you know, why did I do it. There??s something that feels, I suspect, partly that??s a way to not, you know, relate to that anymore, I guess, but I don??t feel like that. You know, I don??t feel, you know, the same, I don??t know, maybe... But I guess in general I feel quite satisfied with that. I mean, I feel, you know, I suppose that you can say there is a process in the world that goes on for a week and then goes back to nothing, but you know, doesn??t -\t\n",
      "THERAPIST: Not that it??s anything more than feeling like we got some closure, if you can say that, but you say, well, I could find some closure if I was just having moments, what does that mean?\t\n",
      "CLIENT: But I??m saying like, you know, if I was just having moments, I would feel kind of grounded and, you know, it would feel real and I would feel, you know... And I suppose that I think the problem is that we don??t have that. We don??t have the kind of intimacy I have...\t\n",
      "THERAPIST: That we need to talk about what we??re talking about is more a social... a therapeutic question about how we??re interacting with, or what we??re talking about...\t\n",
      "CLIENT: It??s a psychological question, I can say it again, but so I guess I just get frustrated and I say, ??Okay, so I guess I am qualified to explain, and I agree with you that my interaction with Jennie??s mother, you know, was kind of a microcosm of many of the issues that she??s face with depression and anxiety and she doesn??t like to say so, you know, or she doesn??t understand. And I don??t like to say so, you know, because I really haven??t been able to articulate it. You know, it??s just just not an issue...\t\n",
      "THERAPIST: In other words, you??re not qualified to explain how your not having a place in your life can mean things that are not in line with you and are being completely unproductive...\t\n",
      "CLIENT: Because Jennie was like, ??I won??t be able to say that. What I??m saying is... I think that??s true. I??m really upset. I??m really upset about things. And, you know, I think this is all, I think this is all about how is it possible to articulate things that were clearly defined at some point ago?\t\n",
      "THERAPIST: Yeah, you could say like, ??Well, I don??t know. When I was at work, if I walked into the\n",
      "\n",
      "[810 | 2687.24] loss=2.08 avg=2.43\n",
      "[820 | 2719.78] loss=2.08 avg=2.43\n",
      "[830 | 2752.31] loss=2.12 avg=2.42\n",
      "[840 | 2784.74] loss=2.11 avg=2.42\n",
      "[850 | 2817.19] loss=1.85 avg=2.41\n",
      "[860 | 2849.62] loss=2.21 avg=2.40\n",
      "[870 | 2882.13] loss=2.15 avg=2.40\n",
      "[880 | 2914.64] loss=2.06 avg=2.39\n",
      "[890 | 2947.10] loss=2.03 avg=2.39\n",
      "[900 | 2979.58] loss=2.28 avg=2.39\n",
      "[910 | 3012.08] loss=2.27 avg=2.38\n",
      "[920 | 3044.60] loss=2.04 avg=2.38\n",
      "[930 | 3077.07] loss=2.21 avg=2.37\n",
      "[940 | 3109.60] loss=2.00 avg=2.37\n",
      "[950 | 3142.11] loss=1.96 avg=2.36\n",
      "[960 | 3174.60] loss=1.89 avg=2.35\n",
      "[970 | 3207.12] loss=1.96 avg=2.35\n",
      "[980 | 3239.56] loss=2.14 avg=2.34\n",
      "[990 | 3272.05] loss=2.06 avg=2.34\n",
      "[1000 | 3304.54] loss=1.94 avg=2.33\n",
      "Saving checkpoint/run1/model-1000\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name='124M',\n",
    "              steps=1000,\n",
    "              restore_from='fresh',\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              sample_every=200,\n",
    "              save_every=500\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXSuTNERaw6K"
   },
   "source": [
    "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
    "\n",
    "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHdTL8NDbAh3"
   },
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQJgV_b4bmzd"
   },
   "source": [
    "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pel-uBULXO2L"
   },
   "source": [
    "## Load a Trained Model Checkpoint\n",
    "\n",
    "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCcx5u7sbPTD"
   },
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTa6zf3e_9gV"
   },
   "source": [
    "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
    "\n",
    "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-fxL77nvAMAX",
    "outputId": "b405ec89-6d8e-427f-fd0d-71343df2db63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model-1000\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClJwpF_ACONp"
   },
   "source": [
    "## Generate Text From The Trained Model\n",
    "\n",
    "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4RNY6RBI9LmL",
    "outputId": "c774098e-5954-417e-a365-a842e4dd11f8"
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value model/h9/attn/c_proj/w\n\t [[{{node model/h9/attn/c_proj/w/read}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5d868bf0f80e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'run1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(sess, run_name, checkpoint_dir, model_name, model_dir, sample_dir, return_as_list, truncate, destination_path, sample_delim, prefix, seed, nsamples, batch_size, length, temperature, top_k, top_p, include_prefix)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             out = sess.run(output, feed_dict={\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value model/h9/attn/c_proj/w\n\t [[node model/h9/attn/c_proj/w/read (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'model/h9/attn/c_proj/w/read':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-67d252685180>\", line 11, in <module>\n    save_every=500\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\", line 198, in finetune\n    output = model.model(hparams=hparams, X=context, gpus=gpus)\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\", line 197, in model\n    h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\", line 156, in block\n    a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\", line 141, in attn\n    a = conv1d(a, 'c_proj', n_state)\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\", line 83, in conv1d\n    w = tf.compat.v1.get_variable('w', [1, nx, nf], initializer=tf.compat.v1.random_normal_initializer(stddev=w_init_stdev))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 1500, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 1243, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 567, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 519, in _true_getter\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 933, in _get_single_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 197, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 2519, in default_variable_creator\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 1688, in __init__\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 1872, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\", line 203, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 4239, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oF4-PqF0Fl7R"
   },
   "source": [
    "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
    "\n",
    "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
    "\n",
    "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
    "\n",
    "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
    "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
    "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
    "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
    "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
    "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "8DKMc0fiej4N",
    "outputId": "171a12db-843f-47b2-fdf9-c66cfb86c9a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.747133493423462\n",
      " It's not that you can't sleep, it's that you can't.\t\n",
      "\n",
      " You can't.\t\n",
      "\n",
      " You're worried?\t\n",
      "\n",
      " You know, it's not a question of, you know, this is what you're going to do. You know, you're going to do what you're going to do. You know, you're going to do what you're going to do. You know, it's not going to\n",
      " You know, that's what I'm trying to say. That's what I'm trying to get at. That's what I'm trying to get at. I'm trying to figure out what's what.\t\n",
      "\n",
      " You can't.\t\n",
      "\n",
      " Mm hm.\t\n",
      "\n",
      " You know, it sounds like you're feeling some kind of anxiety about your work.\t\n",
      "\n",
      " What do you think?\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text=\"\"\"CLIENT: I keep worrying at night. I just can't sleep. What should I do?\n",
    "THERAPIST:\"\"\"\n",
    "\n",
    "numberofsamples=10\n",
    "\n",
    "import time\n",
    "\n",
    "t0=time.time()\n",
    "\n",
    "single_text=gpt2.generate(sess,\n",
    "              length=60,\n",
    "              temperature=.4,\n",
    "              prefix=input_text,\n",
    "              nsamples=numberofsamples,\n",
    "              batch_size=1,\n",
    "              return_as_list=True\n",
    "              )\n",
    "\n",
    "t1=time.time()\n",
    "total=t1-t0\n",
    "print(total)\n",
    "\n",
    "#print(single_text)\n",
    "for i in range(0,(numberofsamples-1)):\n",
    "  print(single_text[i].split(\"THERAPIST:\")[1].split(\"CLIENT:\")[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QvoinG07YfXF"
   },
   "source": [
    "**Create the chatbot!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "nENJRekItLr8",
    "outputId": "bdc911a4-baf9-40db-cc82-6d4e8b8bb8b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you tell me what you want to talk about today? It helps if you say at least a few sentences for context.\n",
      "\n",
      "I've had a very busy day. The winter is making me feel down.\n",
      "\n",
      " Well, I mean maybe we're not talking about job stuff necessarily, but like you said yesterday, it's been a busy week. I mean you've been going over the latest job listings, and it looked like you were leaving a little bit earlier today and leaving a bit later today. So it's been a busy week. And I think it's been a busy week also. I think you've been busy in terms of, you know, just kind of your work/life stuff. You've been really busy with music. You've had a lot of interesting projects. But I think I\n",
      "\n",
      "I've really been applying to jobs.\n",
      "\n",
      " Yeah. That you really should be applying to. And it seems like you've been applying to a lot of different things. Like you're applying to jobs that are really important to you that are not easy to leave alone. Like you're applying to jobs that are really important to you that are not easy to leave alone. Like finding those jobs. Like finding work. Like you have a lot of things going on.\t\n",
      "\n",
      "\n",
      "The job market is really hard. I haven't found anything. I'm about ready to give up. I feel like I'm worthless because I can't do this.\n",
      "\n",
      " Yeah, I guess it's hard to know.\t\n",
      "\n",
      "\n",
      "I'm feeling really down. Like I just can't stop thinking about everything I do that's wrong.\n",
      "\n",
      " Yeah, I guess it's hard to know what to do.\t\n",
      "\n",
      "\n",
      "Are you sure?\n",
      "\n",
      " Yeah, sure. I mean I think you're in for a long week. I think you're in for a long week.\t\n",
      "\n",
      "\n",
      "ABORT\n"
     ]
    }
   ],
   "source": [
    "input_text=\"\"\n",
    "chatbot_response=\"\"\n",
    "numberofsamples=1\n",
    "i=0\n",
    "oldtext=\"\"\n",
    "temp=.8\n",
    "while input_text!=\"ABORT\":\n",
    "  i=i+1\n",
    "  if i==1:\n",
    "    print(\"Can you tell me what you want to talk about today? It helps if you say at least a few sentences for context.\")\n",
    "  input_text = input(\"\\n\")\n",
    "  if input_text==\"ABORT\":\n",
    "    break\n",
    "    \n",
    "\n",
    "  combinedtext=oldtext+\" \"+\"CLIENT: \"+input_text+\"\\nTHERAPIST:\"\n",
    "  single_text=gpt2.generate(sess,\n",
    "              length=120,\n",
    "              temperature=temp,\n",
    "              prefix=combinedtext,\n",
    "              nsamples=numberofsamples,\n",
    "              batch_size=1,\n",
    "              return_as_list=True\n",
    "              )\n",
    "  \n",
    "  #Go Through the batch and see which if the response is greater than 15 characters\n",
    "#  for ii in range(0,(numberofsamples-1)):\n",
    "#    if (len(single_text[ii].split(\"THERAPIST:\")[1].split(\"CLIENT:\")[0])>15):\n",
    "#      single_text[0]=single_text[ii]\n",
    "#      break\n",
    "\n",
    "  temp=.4\n",
    "  while (len(single_text[0].split(\"THERAPIST:\")[i].split(\"CLIENT:\")[0])<15):\n",
    "    temp=temp+.05\n",
    "    single_text=gpt2.generate(sess,\n",
    "              length=120,\n",
    "              temperature=temp,\n",
    "              prefix=combinedtext,\n",
    "              nsamples=numberofsamples,\n",
    "              batch_size=1,\n",
    "              return_as_list=True\n",
    "              )\n",
    "  \n",
    "  oldtext=combinedtext+single_text[0].split(\"THERAPIST: \")[1].split(\"CLIENT:\")[0]\n",
    "  \n",
    "  print(\"\\n\"+single_text[0].split(\"THERAPIST:\")[i].split(\"CLIENT:\")[0]) #change 1 to i if the prior is re-insurted \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmxKbi5Pvixw"
   },
   "outputs": [],
   "source": [
    "print(single_text[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjjEN2Tafhl2"
   },
   "source": [
    "\n",
    "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
    "\n",
    "You can rerun the cells as many times as you want for even more generated texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fa6p6arifSL0"
   },
   "outputs": [],
   "source": [
    "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
    "\n",
    "gpt2.generate_to_file(sess,\n",
    "                      destination_path=gen_file,\n",
    "                      length=500,\n",
    "                      temperature=0.7,\n",
    "                      nsamples=100,\n",
    "                      batch_size=20\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0-LRex8lfv1g"
   },
   "outputs": [],
   "source": [
    "# may have to run twice to get file to download\n",
    "files.download(gen_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQAN3M6RT7Kj"
   },
   "source": [
    "## Generate Text From The Pretrained Model\n",
    "\n",
    "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
    "\n",
    "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsUd_jHgUZnD"
   },
   "outputs": [],
   "source": [
    "model_name = \"774M\"\n",
    "\n",
    "gpt2.download_gpt2(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAe4NpKNUj2C"
   },
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.load_gpt2(sess, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xInIZKaU104"
   },
   "outputs": [],
   "source": [
    "gpt2.generate(sess,\n",
    "              model_name=model_name,\n",
    "              prefix=\"The secret of life is\",\n",
    "              length=100,\n",
    "              temperature=0.7,\n",
    "              top_p=0.9,\n",
    "              nsamples=5,\n",
    "              batch_size=5\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ig-KVgkCDCKD"
   },
   "source": [
    "# Etcetera\n",
    "\n",
    "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIHiVP53FnsX"
   },
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmTXWNUygS5E"
   },
   "source": [
    "# LICENSE\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 Max Woolf\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "chatbot_context_train",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
