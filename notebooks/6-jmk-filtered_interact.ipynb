{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6-jcb-filtered_interact.ipynb","provenance":[],"authorship_tag":"ABX9TyMrqPQngxJmF1UvQOWa7VUa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6xfA6RiW1p5j","colab_type":"code","outputId":"995e7af1-ac80-42f3-84a4-68f0af4586d9","executionInfo":{"status":"ok","timestamp":1588209460135,"user_tz":240,"elapsed":76013,"user":{"displayName":"Jason Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64","userId":"07126822508397489067"}},"colab":{"base_uri":"https://localhost:8080/","height":887}},"source":["!pip install empath\n","!pip install vaderSentiment\n","!pip install plydata\n","\n","from plydata import *\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize \n","\n","from collections import defaultdict, Counter\n","from empath import Empath\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","\n","%tensorflow_version 1.x\n","!pip install -q gpt-2-simple\n","import gpt_2_simple as gpt2\n","from datetime import datetime\n","from google.colab import files\n","from google.colab import drive\n","import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from joblib import load\n","import pickle\n","import random\n","from itertools import tee, islice\n","import re\n","\n","# Numba\n","from numba import jit, njit, vectorize, cuda\n","\n","# Mount drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting empath\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/84/a5de61a99252f60d705d7982b3648db517a704c89fa7629d3d3637a6e604/empath-0.89.tar.gz (57kB)\n","\r\u001b[K     |█████▊                          | 10kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.4MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from empath) (2.21.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->empath) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->empath) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->empath) (2020.4.5.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->empath) (2.8)\n","Building wheels for collected packages: empath\n","  Building wheel for empath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for empath: filename=empath-0.89-cp36-none-any.whl size=57824 sha256=9ab2dc2e74f8b0dd8611c1f078d050dea7e89ef757fe1ff5dced141962428646\n","  Stored in directory: /root/.cache/pip/wheels/84/ea/2f/2bc54d4f9985ce61753ebc5b00cb2df51d855589267c667308\n","Successfully built empath\n","Installing collected packages: empath\n","Successfully installed empath-0.89\n","Collecting vaderSentiment\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/a3/1218a3b5651dbcba1699101c84e5c84c36cbba360d9dbf29f2ff18482982/vaderSentiment-3.3.1-py2.py3-none-any.whl (125kB)\n","\u001b[K     |████████████████████████████████| 133kB 9.0MB/s \n","\u001b[?25hInstalling collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.1\n","Collecting plydata\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/76/4e7f5c31da1c06c2798ddceaf9626da5fccbcceb9056eb93cf5492e2e8e5/plydata-0.4.0-py3-none-any.whl (299kB)\n","\u001b[K     |████████████████████████████████| 307kB 8.8MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from plydata) (1.0.3)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->plydata) (1.18.3)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->plydata) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->plydata) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=1.0.0->plydata) (1.12.0)\n","Installing collected packages: plydata\n","Successfully installed plydata-0.4.0\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","TensorFlow 1.x selected.\n","  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RoDPwBLsCSLK","colab_type":"code","colab":{}},"source":["# Set up for numba\n","os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/local/cuda-10.1/nvvm/libdevice\"\n","os.environ['NUMBAPRO_NVVM'] = \"/usr/local/cuda-10.1/nvvm/lib64/libnvvm.so\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T0ZXC2hq2dop","colab_type":"text"},"source":["# Import and Load Trained GPT2 Model"]},{"cell_type":"code","metadata":{"id":"MvD-SMlRJJfU","colab_type":"code","outputId":"92062185-e010-47b9-c471-a82a1609a9c7","executionInfo":{"status":"ok","timestamp":1588209486011,"user_tz":240,"elapsed":28703,"user":{"displayName":"Jason Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64","userId":"07126822508397489067"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Copy checkpoint\n","gpt2.copy_checkpoint_from_gdrive(run_name='run1')\n","\n","# Start session and load model\n","sess = gpt2.start_tf_sess()\n","gpt2.load_gpt2(sess, run_name='run1', multi_gpu=True)\n","\n","# Import scaler\n","scaler = pickle.load(open('/content/drive/My Drive/5_datascience/2_context_chatbot/data/models/scaler.p', 'rb'))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Loading checkpoint checkpoint/run1/model-1000\n","INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6ij8ZSMgYhhk","colab_type":"code","colab":{}},"source":["# calc_semantic_similarity(test_df.loc[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Att_Ky74YdlD","colab_type":"code","colab":{}},"source":["# Calculate ngrams as a mechansim for filtering out poor responses, which oftentimes repeated phrases.\n","def calc_trigrams(text):\n","    text = text.lower()\n","    # Tokenize the input text\n","    tokens = nltk.word_tokenize(text)\n","    \n","    # Create trigrams\n","    tgs = nltk.trigrams(tokens)\n","\n","    # Compute frequency distribution for all the ngrams in the text\n","    fdist = nltk.FreqDist(tgs)\n","    \n","    return(fdist)\n","\n","def trigram_present(ngram_fdist):\n","    for k,v in ngram_fdist.items():\n","        if v > 1:\n","            return True\n","    return False\n","\n","def max_tgram_freq(ngram_fdist):\n","    max_value = 0\n","    for k,v in ngram_fdist.items():\n","        if v > max_value:\n","            max_value = v\n","    return v\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFjpWm6nYxvM","colab_type":"code","colab":{}},"source":["# test_grams = calc_trigrams(\"Jason is the best in the world. Just in case you think different, fuck you.\")\n","# max_tgram_freq(test_grams)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBjrvFdI3F2T","colab_type":"code","colab":{}},"source":["def add_prefix_to_dict_keys(dict, prefix):\n","    return {f'{prefix}_{k}': v for k, v in dict.items()}\n","\n","# calc similarity was below here \n","def calc_semantic_similarity(prompt_response_row_df):\n","    x = prompt_response_row_df['prompt']\n","    y = prompt_response_row_df['response']\n","\n","    # tokenization first\n","    x_list = word_tokenize(x)\n","    y_list = word_tokenize(y)\n","\n","    # Check for stop words\n","    sw = stopwords.words('english')  \n","    l1 = list()\n","    l2 = list()\n","\n","    # Remove stop words from string\n","    x_set = {w for w in x_list if not w in sw}  \n","    y_set = {w for w in y_list if not w in sw}  \n","\n","\n","    # form a set containing keywords of both strings  \n","    rvector = x_set.union(y_set) \n","    for w in rvector: \n","        if w in x_set: l1.append(1) # create a vector \n","        else: l1.append(0) \n","        if w in y_set: l2.append(1) \n","        else: l2.append(0)  \n","    \n","    l1_arr = np.array(l1)\n","    l2_arr = np.array(l2)\n","\n","    return calc_cosine(l1_arr, l2_arr, len(rvector))\n","\n","\n","@njit(cache=True)\n","def calc_cosine(l1_arr, l2_arr, len_rvector):\n","\n","    c = 0\n","    for i in range(len_rvector): \n","        c+= l1_arr[i]*l2_arr[i] \n","    cosine = c / float((np.sum(l1_arr)*np.sum(l2_arr))**0.5) \n","    return cosine\n","\n","\n","class SeanceScorer():\n","\n","    def __init__(self):\n","        # add in inquiererbasic.txt\n","        with open('/content/drive/My Drive/5_datascience/2_context_chatbot/data/inputs/seance/inquirerbasic.txt', 'r') as content_file:\n","            content = content_file.read()\n","\n","        split_by_metric = content.split('\\n')\n","\n","        self.seance_dict = {}\n","        for line in split_by_metric:\n","            word_list = line.split('\\t')\n","            self.seance_dict[f'{word_list[0]}'] = word_list[1:]\n","\n","        # add in negative_words.txt\n","        with open('/content/drive/My Drive/5_datascience/2_context_chatbot/data/inputs/seance/negative_words.txt', 'r') as content_file:\n","            content = content_file.read()\n","\n","        self.seance_dict['negative_words'] = content.split('\\n')\n","\n","        # add in positive_words.txt\n","        with open('/content/drive/My Drive/5_datascience/2_context_chatbot/data/inputs/seance/positive_words.txt', 'r') as content_file:\n","            content = content_file.read()\n","\n","        self.seance_dict['positive_words'] = content.split('\\n')\n","    \n","\n","    def calc_seance_scores(self, text_to_score, type_scored):\n","        scores_dict = {}\n","        for dict_key in self.seance_dict.keys():\n","            cur_list = self.seance_dict[dict_key] # The list of words associated with the seance feature\n","            counted = Counter(text_to_score.split())\n","            common_words = set(cur_list).intersection(counted)\n","            count_common = sum(counted[wrd] for wrd in set(cur_list))\n","            scores_dict[f'{type_scored}_s_{dict_key}'] = count_common\n","        return(scores_dict)\n","\n","def choose_optimal_reponse(responses_list):\n","    pass\n","\n","# myround stuff was here \n","def myround(x, base=5):\n","    return base * round(x/base)\n","\n","\n","def score_senti_prompts_responses(prompt_scores_df, seance_scorer, lexicon, analyzer, take_out_speaker_in_prompt=True):\n","    all_scores_list = list()\n","    for i in range(len(prompt_scores_df)):\n","        prompt = prompt_scores_df['prompt'][i]\n","        if take_out_speaker_in_prompt:\n","            prompt = prompt.replace('CLIENT:', '')\n","            prompt = prompt.replace('THERAPIST:', '')\n","        response = prompt_scores_df['response'][i]\n","        seance_input_scores = pd.DataFrame(seance_scorer.calc_seance_scores(prompt, type_scored='prompt'), index=[i])\n","        seance_output_scores = pd.DataFrame(seance_scorer.calc_seance_scores(response, type_scored='response'), index=[i])\n","        empath_input_scores = pd.DataFrame(add_prefix_to_dict_keys(lexicon.analyze(prompt), prefix='prompt_e'), index=[i])\n","        empath_output_scores = pd.DataFrame(add_prefix_to_dict_keys(lexicon.analyze(response), prefix='response_e'), index=[i])\n","        vader_input_scores = pd.DataFrame(add_prefix_to_dict_keys(analyzer.polarity_scores(prompt), prefix='prompt_v'), index=[i])\n","        vader_output_scores = pd.DataFrame(add_prefix_to_dict_keys(analyzer.polarity_scores(response), prefix='response_v'), index=[i])\n","\n","        all_scores_list.append(pd.concat([seance_input_scores, seance_output_scores, empath_input_scores, empath_output_scores, vader_input_scores, vader_output_scores], axis=1))\n","\n","    all_scores = pd.concat(all_scores_list)\n","    return(all_scores)\n","\n","\n","def score_generated_responses(responses_df):\n","    # initializes three of the scorers\n","    seance_scorer = SeanceScorer() # from custom made seance scorer\n","    lexicon = Empath() # from empath library\n","    analyzer = SentimentIntensityAnalyzer()\n","\n","    all_scores_df = score_senti_prompts_responses(responses_df, seance_scorer, lexicon, analyzer)\n","    all_scores_df['similarity_score'] = responses_df.apply(calc_semantic_similarity, axis=1)\n","    return all_scores_df\n","\n","\n","def generate_unique_responses_for_prompt(input_text, num_responses, verbose=0):\n","    \"\"\" Generates a dataframe with repeated input texts, num_responses outputs, and temp values.\n","\n","    Keyword arguments:\n","    input_text -- The string that is inputted into the gp2 text generative model.\n","    num_responses -- Desired number of unique responses. \n","\n","    \"\"\"\n","    temp = .4\n","    data_dict_list = list()\n","\n","    # Generate text from the combined input text\n","    while len(data_dict_list) < num_responses:\n","        nsamples = num_responses - len(data_dict_list)\n","        nsamples = myround(nsamples, 20) + 20\n","        generated_texts = gpt2.generate(sess,\n","                  length=120,\n","                  temperature=temp,\n","                  prefix=input_text,\n","                  nsamples=nsamples,\n","                  batch_size=20,\n","                  return_as_list=True\n","                  )\n","\n","        # check if the outputs meet the requirements. \n","        for k in range(len(generated_texts)):\n","        \n","            # Check if the response has a 'THERAPIST: in the generated response' \n","            if len(generated_texts[k].split(input_text)[1].split('THERAPIST:')) > 0:\n","                sample_response = generated_texts[k].split(input_text)[1].split('CLIENT:')[0].strip()\n","                tgram_fdis = calc_trigrams(sample_response)\n","\n","                # Check if the length of the possible response is greater than 1 \n","                # and add the sample response \n","                if len(sample_response) > 1 and len(data_dict_list) < num_responses and not trigram_present(tgram_fdis):\n","                    # Checks for if the sample response has already been inputted into the list. \n","                    if not any(d['response'] == sample_response for d in data_dict_list): \n","                        data_dict = dict()\n","                        data_dict['temp'] = temp\n","                        data_dict['prompt'] = input_text\n","                        data_dict['response'] = sample_response\n","\n","                        data_dict_list.append(data_dict)\n","\n","\n","        temp += .05 # randomness parameter\n","\n","        if verbose > 0:\n","            print(f\"{len(data_dict_list)} out of {num_responses} responses have been generated.\")\n","\n","    data_df = pd.DataFrame(data_dict_list)\n","    return(data_df)\n","\n","\n","def interact_with_bot(moving_window_used, moving_window=1, min_len_response=15, remove_responses=False, num_samples = 1, verbose=0):\n","\n","    # initialize inputs and chatbot reponses\n","    input_text = \"\"\n","    chatbot_response = \"\"\n","    i = 0\n","    oldtext = \"\"\n","    temp = .8 # what does temp do?\n","\n","    # Initialize lists to store the prompts and responses.\n","    client_prompts = list()\n","    therapist_responses = list()\n","    interaction_history = list()\n","\n","    # Initialize the filtering model\n","    rf_model = load('/content/drive/My Drive/5_datascience/2_context_chatbot/data/models/default_rf.joblib')\n","\n","    while input_text!=\"ABORT\" or input_text!= \"SAVE\" :\n","        i = i + 1        \n","        if i==1:\n","\n","            print(\"Can you tell me what you want to talk about today? It helps if you say at least a few sentences for context.\")\n","        input_text = input(\"\\n\")\n","        client_prompts.append(input_text) # adds the user input to prompts list\n","\n","        if input_text==\"ABORT\":\n","            break\n","        elif input_text==\"SAVE\":\n","            save_df = pd.DataFrame(interaction_history)\n","            return(save_df)\n","\n","        if(remove_responses == False):\n","        # Filter the input text so that it fits into the moving window\n","            if(len(client_prompts) <= moving_window):\n","                combined_text = \"\"\n","                for j in range(len(client_prompts)):\n","                    if j != 0:\n","                        combined_text = combined_text + therapist_responses[j - 1]\n","\n","                    combined_text = combined_text + \"\\nCLIENT: \" + client_prompts[j] + \"\\nTHERAPIST:\" \n","            else: \n","                moving_window_used = True\n","                combined_text = \"\"\n","                prompts_subset = client_prompts[-moving_window:]\n","                responses_subset = therapist_responses[-moving_window + 1:]\n","\n","                for j in range(len(prompts_subset)):\n","                    if j != 0:\n","                        combined_text = combined_text + responses_subset[j - 1]\n","                    combined_text = combined_text + \"\\nCLIENT: \" + prompts_subset[j] + \"\\nTHERAPIST:\" \n","\n","        else: \n","            combined_text = \"Client: \" + ' '.join(client_prompts[-moving_window:])\n","\n","        combined_text = combined_text.strip()\n","        if verbose > 0:\n","            print(f'\\nGPT2 input text is ... \\n{combined_text} \\n')\n","\n","\n","        # Generate unique responses for a given input \n","        # change input_text=combined_text to use moving window\n","        responses_df = generate_unique_responses_for_prompt(input_text= combined_text, num_responses=100, verbose=verbose)\n","\n","        # Score the unique responses and use a random forest model to estimate how useful a given response is.\n","        X = pd.concat([score_generated_responses(responses_df), responses_df['temp']], axis=1)\n","        X = X.reindex(sorted(X.columns), axis=1) \n","        X_scaled = scaler.transform(X)\n","        y_pred = pd.Series(rf_model.predict(X_scaled))\n","        y_pred = y_pred.rename('y_pred')\n","        responses_preds_df = pd.concat([responses_df, y_pred], axis=1)\n","        responses_preds_df = responses_preds_df.sort_values(by='y_pred', ascending=False)\n","        best_response = responses_preds_df.iloc[0]['response']\n","        interaction_history.append(responses_preds_df.loc[0])\n","        therapist_responses.append(best_response)\n","        if verbose > 0:\n","            print(f\"The top 5 potential responses are: \\n\")\n","            top_5_df = responses_preds_df.head(5)\n","            for index, row in top_5_df.iterrows():\n","                print(f\"{row['y_pred']}: {row['response']}\")\n","    \n","        print(\"\\nTherapist Bot: \"+ best_response) #change 1 to i if the prior is re-insurted \n","\n","     "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKfZ8mfzeFq7","colab_type":"code","outputId":"ce9c8bde-cc77-4279-9a1a-49be5dba94b3","executionInfo":{"status":"ok","timestamp":1588187294769,"user_tz":240,"elapsed":343961,"user":{"displayName":"Jason Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64","userId":"07126822508397489067"}},"colab":{"base_uri":"https://localhost:8080/","height":818}},"source":["saved_history = interact_with_bot(verbose=1, moving_window_used=True)\n","saved_history"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Can you tell me what you want to talk about today? It helps if you say at least a few sentences for context.\n","\n","I am very hungry right now. \n","\n","GPT2 input text is ... \n","CLIENT: I am very hungry right now. \n","THERAPIST: \n","\n","37 out of 100 responses have been generated.\n","56 out of 100 responses have been generated.\n","70 out of 100 responses have been generated.\n","91 out of 100 responses have been generated.\n","97 out of 100 responses have been generated.\n","100 out of 100 responses have been generated.\n","The top 5 potential responses are: \n","\n","3.95: I wonder if there's a way you can kind of tease them apart a little bit.\n","3.66: I wonder if it's hard to talk about it.\n","3.39: I wonder if it's a little bit about the way you're feeling right now.\n","3.345: Yeah. I mean I think that's part of what we're talking about is not just your hunger, but your fear that you could get something for her.\n","3.31: That's good.\n","\n","Therapist Bot: I wonder if there's a way you can kind of tease them apart a little bit.\n","\n","Life is super dull and boring\n","\n","GPT2 input text is ... \n","CLIENT: Life is super dull and boring\n","THERAPIST: \n","\n","64 out of 100 responses have been generated.\n","90 out of 100 responses have been generated.\n","97 out of 100 responses have been generated.\n","100 out of 100 responses have been generated.\n","The top 5 potential responses are: \n","\n","3.62: It's not that you're not going to get to write a book.\n","3.57: It's not that your life is boring. It's that you're not that interested in it.\n","3.54: That's the way I understand it.\n","3.53: It's not like you're doing anything new or interesting.\n","3.5: It's a life you're living.\n","\n","Therapist Bot: It's not that you're not going to get to write a book.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cblxNW2S79Sp","colab_type":"code","outputId":"36972a67-3b1f-47c9-e53a-a92c7aad3747","executionInfo":{"status":"ok","timestamp":1588178266963,"user_tz":240,"elapsed":32374,"user":{"displayName":"Jason Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64","userId":"07126822508397489067"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["test_string = 'CLIENT: Hello I am always feeling so tired. What a waste of life.\\nTHERAPIST:'\n","test_df = generate_unique_responses_for_prompt(test_string, num_responses=20, verbose=1)\n","X = pd.concat([score_generated_responses(test_df), test_df['temp']], axis=1)\n","X = X.reindex(sorted(X.columns), axis=1)\n","X = scaler.transform(X)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["16 out of 20 responses have been generated.\n","20 out of 20 responses have been generated.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ajDzqP5jP65d","colab_type":"code","colab":{}},"source":["test_list = []\n","test_list.append(test_df.loc[0])\n","test_list.append(test_df.loc[0])\n","\n","test_result = pd.DataFrame(test_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"18YpYlxWyW_i","colab_type":"code","outputId":"504025d8-5e1a-4985-c520-297c9afcbecb","executionInfo":{"status":"ok","timestamp":1588178266965,"user_tz":240,"elapsed":31933,"user":{"displayName":"Jason Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64","userId":"07126822508397489067"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["test_df.loc[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["temp                                                      0.4\n","prompt      CLIENT: Hello I am always feeling so tired. Wh...\n","response                                It's a waste of life.\n","Name: 0, dtype: object"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"code","metadata":{"id":"AeDTf1MQW7IZ","colab_type":"code","outputId":"d595441b-72e3-498c-d22e-1497347fb2c7","executionInfo":{"status":"ok","timestamp":1588178266968,"user_tz":240,"elapsed":28480,"user":{"displayName":"Jason Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64","userId":"07126822508397489067"}},"colab":{"base_uri":"https://localhost:8080/","height":110}},"source":["test_result"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>temp</th>\n","      <th>prompt</th>\n","      <th>response</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.4</td>\n","      <td>CLIENT: Hello I am always feeling so tired. Wh...</td>\n","      <td>It's a waste of life.</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>0.4</td>\n","      <td>CLIENT: Hello I am always feeling so tired. Wh...</td>\n","      <td>It's a waste of life.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   temp  ...               response\n","0   0.4  ...  It's a waste of life.\n","0   0.4  ...  It's a waste of life.\n","\n","[2 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"markdown","metadata":{"id":"-8MJ0mWgyi7b","colab_type":"text"},"source":["%timeit myround_njit(random.randint(1,100))"]},{"cell_type":"code","metadata":{"id":"A4mCYAzD8Hrz","colab_type":"code","outputId":"71af3955-8c30-4cf8-cb97-8e9676edae4f","executionInfo":{"status":"error","timestamp":1588046152592,"user_tz":240,"elapsed":11853,"user":{"displayName":"Jason Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64","userId":"07126822508397489067"}},"colab":{"base_uri":"https://localhost:8080/","height":249}},"source":["test_string = 'CLIENT: Hello I am always feeling so tired. What a waste of life.\\nTHERAPIST:'\n","test_df = generate_unique_responses_for_prompt(test_string, num_responses=20, verbose=1)\n","X = pd.concat([score_generated_responses(test_df), test_df['temp']], axis=1)\n","X = X.reindex(sorted(X.columns), axis=1)\n","X = scaler.transform(X)\n","\n","rf_model = load('/content/drive/My Drive/5_datascience/2_context_chatbot/data/models/default_rf.joblib')\n","y_pred = pd.Series(predict_and_rank(X, rf_model))\n","y_pred = y_pred.rename('y_pred')\n","responses_preds_df = pd.concat([test_df, y_pred], axis=1)\n","responses_preds_df = responses_preds_df.sort_values(by='y_pred', ascending=False)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["20 out of 20 responses have been generated.\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-0a8bd630ff63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/5_datascience/2_context_chatbot/data/models/default_rf.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_and_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresponses_preds_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'predict_and_rank' is not defined"]}]},{"cell_type":"code","metadata":{"id":"ybH9mbnBY3bS","colab_type":"code","colab":{}},"source":["responses_preds_df = responses_preds_df.sort_values(by='y_pred', ascending=False)\n","best_response = responses_preds_df.iloc[0]['response']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pN6m-V7nR6Cj","colab_type":"code","outputId":"83eab530-421c-40ea-d05f-b69adf45583c","executionInfo":{"status":"ok","timestamp":1587567417206,"user_tz":240,"elapsed":1637,"user":{"displayName":"Jason Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64","userId":"07126822508397489067"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["best_response"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"You're a real person.\""]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"24R-qe2SSpFu","colab_type":"code","outputId":"7bc63264-f60c-471f-fce8-2ac68fa4253b","executionInfo":{"status":"ok","timestamp":1587565746686,"user_tz":240,"elapsed":1781,"user":{"displayName":"Jason Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiK8IafaB0Bu0wQRn-pJyDgCzHNv6GObP38rUvzzXc=s64","userId":"07126822508397489067"}},"colab":{"base_uri":"https://localhost:8080/","height":242}},"source":[""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.07449132, 0.        , ..., 0.00499092, 0.03583965,\n","        0.02979653],\n","       [0.        , 0.04646402, 0.        , ..., 0.00218381, 0.00948443,\n","        0.01858561],\n","       [0.        , 0.07037979, 0.        , ..., 0.01309064, 0.01225155,\n","        0.02815192],\n","       ...,\n","       [0.        , 0.11327224, 0.        , ..., 0.        , 0.0231216 ,\n","        0.04530889],\n","       [0.        , 0.02337794, 0.        , ..., 0.00032729, 0.00441801,\n","        0.00935118],\n","       [0.        , 0.0897509 , 0.        , ..., 0.        , 0.01495848,\n","        0.03590036]])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"vaF3lRCsNQbh","colab_type":"code","colab":{}},"source":["# Split the data into features and labels. \n","X = all_scored_full >> select('-input','-output_list', '-NCJ_Rating')\n","X = X.reindex(sorted(X.columns), axis=1)\n","y = all_scored_full['NCJ_Rating']\n","\n","# Normalize the features\n","X = normalize(X)"],"execution_count":0,"outputs":[]}]}